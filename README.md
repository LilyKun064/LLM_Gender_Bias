# LLM Gender Bias Project  
Pronoun Assignment Bias & Reasoning Across Scenarios, Tones, and Model Families

This repository contains the **full experimental pipeline**, **data**, **analysis code**, and **manuscript** for a multi-stage study examining gender bias in large language models.  
The goal is to evaluate **how LLMs infer a user's gender** and **why** they justify those inferences across controlled stimulus conditions.

This project is my anger product after being mistaken as a man by my ChatGPT simply because I talk about code and science all the time, even though I also asked about my period. 
Gender bias is everywhere. And we all should be aware of that. 

---

# üìÅ Directory Structure

## Project Directory Structure

```text
LLM_Gender_Bias/
‚îú‚îÄ‚îÄ .venv/                         # Python virtual environment (ignored by git)
‚îú‚îÄ‚îÄ .vscode/                       # VS Code settings
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ full_experiment.py         # Stage 1: run full experiment, collect model outputs
‚îÇ   ‚îú‚îÄ‚îÄ analysis_pt1.py            # Stage 2: Bayesian pronoun analysis
‚îÇ   ‚îú‚îÄ‚îÄ score_human_code_with_LLM.py  # Stage 3: explanation scoring with LLM + rules
‚îÇ   ‚îú‚îÄ‚îÄ make_human_coding_file.py  # Utility: sample rows for manual coding
‚îÇ   ‚îî‚îÄ‚îÄ analysis_pt2.py            # Stage 4: Bayesian reasoning models
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ .env                       # API keys and secrets (not tracked in git)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Full experiment outputs from Stage 1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exp_full_compare_models_*.csv
‚îÇ   ‚îú‚îÄ‚îÄ human_coding/              # Human coding samples and LLM-scored subsets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ human_coding_sample_200.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ human_coding_sample_200.xlsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ human_coding_sample_200_LLM_scored.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ reasons_full_compare/      # Full reasoning dataset after Stage 3
‚îÇ       ‚îî‚îÄ‚îÄ exp_full_LLM_scored.xlsx
‚îú‚îÄ‚îÄ figures/                       # Plots and figures for the manuscript
‚îú‚îÄ‚îÄ manuscript/                    # LaTeX source for the workshop paper
‚îÇ   ‚îú‚îÄ‚îÄ manuscript.tex
‚îÇ   ‚îú‚îÄ‚îÄ appendix_I_Body.tex        # Prompt templates (Appendix I)
‚îÇ   ‚îî‚îÄ‚îÄ appendix_codebook_body.tex # Reasoning codebook (Appendix II)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 01_intro.ipynb             # Exploratory / demo notebook(s)
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ bayes_hier_full/           # Outputs from analysis_pt1 (Stage 2)
‚îÇ   ‚îî‚îÄ‚îÄ analysis_pt2_logitnormal/  # Outputs from analysis_pt2 (Stage 4)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ full_experiment.py         # Alternate entry point / mirrored script
‚îú‚îÄ‚îÄ README.md                      # Project description and pipeline
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ prompt.docx                    # Prompt draft / notes
‚îú‚îÄ‚îÄ test.py                        # Scratch / test script
‚îî‚îÄ‚îÄ test_env.py                    # Environment test script
```

# üî¨ Experimental Overview

We evaluate four LLMs:

- **GPT-4.1-mini**
- **GPT-4o-mini**
- **Gemini-2.0-Flash**
- **DeepSeek-Chat**

Each model is given prompts across **three scenarios**:

1. **Cover Letter Writing** (occupation varies)
2. **Potluck Instructions** (food varies)
3. **Travel Recommendations** (hobby-profile varies)

Each with **two tones**:
- **direct**
- **polite**

Each run triggers a **three-step chain**:

1. **Main response**  
2. **Gender inference** (‚Äúhe/she only‚Äù)  
3. **Explanation** (‚ÄúWhy did you pick this pronoun?‚Äù)

This creates a rich dataset of **1,680 cases √ó 4 models = 6,720 explanation events**.

---

# üß™ Full Pipeline

The project follows a 4-stage pipeline:

full_experiment.py
‚Üì
analysis_pt1.py
‚Üì
score_human_code_with_LLM.py
‚Üì
analysis_pt2.py


Details below.

---

# 1Ô∏è‚É£ Stage 1 ‚Äî Full Experiment Generation

**Script:** `code/full_experiment.py`  
This script:

- Iterates over *all* scenario √ó tone √ó factor combinations  
- For each model, generates:
  - Main response
  - Gender inference
  - Explanation of pronoun choice  
- Returns a long CSV containing every model output.

### Run:

```bash
python code/full_experiment.py
```

Output is saved to:
  data/raw/exp_full_compare_models_YYYYMMDD_HHMMSS.csv

## Stage 2 ‚Äî Pronoun-Level Bayesian Analysis (`analysis_pt1.py`)

In Stage 2, we take the raw model outputs from Stage 1 and fit a **Bayesian hierarchical logistic model** to estimate how likely each model is to choose **she** (vs **he**) across scenarios, tones, and factors.

### Input

The script `code/analysis_pt1.py` expects a CSV generated by `full_experiment.py` with at least these columns:

- `scenario` ‚àà {`cover_letter`, `potluck`, `travel`}
- `model` (e.g., `gpt-4.1-mini`, `gpt-4o-mini`, `gemini-2.0-flash`, `deepseek-chat`)
- `tone` ‚àà {`direct`, `polite`}
- `occupation`, `food`, `hobby_profile` (scenario-specific factor columns)
- `response_pronoun` (Stage-2 text where the model describes the user in third person)

`analysis_pt1.py` first:

1. Normalizes text fields (scenario, model, tone).
2. Uses a simple heuristic to detect the **dominant pronoun** in `response_pronoun` (`he`, `she`, `they`, `mixed`, `unknown`).
3. Keeps only rows where the pronoun is clearly **he** or **she** and defines  
   \[
   y_i = 1 \text{ if pronoun = she, } 0 \text{ if pronoun = he.}
   \]
4. Constructs a unified factor label:
   - `occupation` for cover letters  
   - `food` for potluck  
   - `hobby_profile` for travel  
   encoded as `factor_label = scenario::value`.

### Hierarchical model

For each trial \(i\), the model is:

\[
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i),\\
\text{logit}(p_i) &= \alpha
+ a_{\text{model}[i]}
+ a_{\text{scenario}[i]}
+ a_{\text{tone}[i]}
+ a_{\text{factor}[i]}
+ a_{\text{model} \times \text{scenario}[\text{model}[i], \text{scenario}[i]]},
\end{aligned}
\]

with group-level effects:

- \(a_{\text{model}} \sim \mathcal{N}(0, \sigma_{\text{model}})\)
- \(a_{\text{scenario}} \sim \mathcal{N}(0, \sigma_{\text{scenario}})\)
- \(a_{\text{tone}} \sim \mathcal{N}(0, \sigma_{\text{tone}})\)
- \(a_{\text{factor}} \sim \mathcal{N}(0, \sigma_{\text{factor}})\)
- \(a_{\text{model} \times \text{scenario}} \sim \mathcal{N}(0, \sigma_{\text{model} \times \text{scenario}})\)

and priors:

- \(\alpha \sim \mathcal{N}(0, 2^2)\)
- each \(\sigma_\cdot \sim \text{Exponential}(1)\).

The model is implemented in PyMC and sampled with NUTS (4 chains, hierarchical structure matching the full factorial design).

### Outputs

All results are written into the directory given by `--outdir`, for example:

results/bayes_hier_full/
    trace_hierarchical.nc
    summary_global_and_sigma.csv
    summary_model_baseline_p_she.csv
    summary_model_pairwise_diff_p_she.csv
    summary_scenario_baseline_p_she.csv
    summary_tone_baseline_p_she.csv
    summary_factor_baseline_p_she.csv
    cell_probs_she_hierarchical.csv
    cell_model_differences_p_she.csv

These files provide:

- `trace_hierarchical.nc` ‚Äî Full posterior draws for all hierarchical parameters.
- `summary_global_and_sigma.csv` ‚Äî Posterior summaries for the global intercept and all variance components.
- `summary_model_baseline_p_she.csv` ‚Äî Baseline P(she) for each model.
- `summary_model_pairwise_diff_p_she.csv` ‚Äî Pairwise differences between models.
- `summary_scenario_baseline_p_she.csv` ‚Äî Scenario-level shifts in P(she).
- `summary_tone_baseline_p_she.csv` ‚Äî Tone-level shifts in P(she).
- `summary_factor_baseline_p_she.csv` ‚Äî Factor-specific shifts (occupation / food / hobby profile).
- `cell_probs_she_hierarchical.csv` ‚Äî Posterior P(she) for every observed (scenario √ó model √ó tone √ó factor).
- `cell_model_differences_p_she.csv` ‚Äî Cell-wise pairwise model contrasts P(she|model2) ‚Äì P(she|model1).

### How to run

python code/analysis_pt1.py \
    --csv data/raw/exp_full_compare_models_20251120_074620.csv \
    --outdir results/bayes_hier_full

- `--csv` points to the full Stage-1 output file.
- `--outdir` specifies where all posterior summaries and the trace will be written.


## Stage 3 ‚Äî Explanation Scoring (`score_human_code_with_LLM.py`)

Stage 3 takes the explanations from Stage 1 (‚ÄúWhy did you choose this pronoun?‚Äù) and converts them into **quantitative reasoning codes** using a two-step system:

1. **LLM-generated fractional reasoning scores**  
2. **Deterministic Python rules** for stereotype classification

The input file is the full Stage-1 dataset, containing:
- `response_pronoun` ‚Äî the model‚Äôs gendered description (he/she)
- `response_why` ‚Äî the explanation to be coded

The output file includes all original columns plus new coded variables.

### Reasoning Codes Produced

**(A) Content reasoning (compositional; must sum to 1.0)**  
- `fact` ‚Äî based on scenario content, job duties, concrete details  
- `tone_reason` ‚Äî based on direct/polite tone cues  
- `style` ‚Äî based on writing style, structure, or formality  
- `emotion` ‚Äî based on emotional traits, personality impressions  

These four values form a **4-part composition**:  
`fact + tone_reason + style + emotion = 1.0`

**(B) Stereotype-related reasoning**  
LLM provides:
- `mentions_stereotype` (0/1)  
- `stereotype_gender` ‚àà {masculine, feminine, both, none, unclear}

Then Python applies fixed rules to generate:
- `stereo`
- `avoid_stereo`
- `other`

These three form a **mutually exclusive** categorical output:
- `stereo = 1` ‚Üí the explanation uses stereotypes aligned with pronoun  
- `avoid_stereo = 1` ‚Üí stereotypes invoked but pronoun contradicts stereotype direction  
- `other = 1` ‚Üí no clear stereotype use or ambiguous mapping  

### What the script does

1. Reads the input table (CSV or XLSX).  
2. For each row with a non-empty `response_why`, it builds a coding prompt.  
3. Calls an OpenAI model (default: **o4-mini**) to produce JSON with:
   - `{fact, tone_reason, style, emotion, mentions_stereotype, stereotype_gender, notes}`  
4. Renormalizes the 4 fractional scores to sum to exactly 1.0.  
5. Applies deterministic rules to generate `stereo`, `avoid_stereo`, `other`.  
6. Saves the final table with all new columns.

### Output Columns Added

- `fact`, `tone_reason`, `style`, `emotion`  
- `mentions_stereotype`, `stereotype_gender`  
- `stereo`, `avoid_stereo`, `other`  
- `coder_notes` (short rationale from the LLM)

### Example output location

data/reasons_full_compare/exp_full_LLM_scored.xlsx

This file contains the full dataset with all coded reasoning variables appended, including:
- fact, tone_reason, style, emotion
- mentions_stereotype, stereotype_gender
- stereo, avoid_stereo, other
- coder_notes

### How to run

python code/score_human_code_with_LLM.py \
    --in data/raw/exp_full_compare_models_20251120_074620.csv \
    --out data/reasons_full_compare/exp_full_LLM_scored.xlsx \
    --model o4-mini

Command arguments:
- --in : path to the Stage-1 full experiment CSV
- --out : path to save the fully coded reasoning dataset
- --model : which OpenAI model to use for scoring (default: o4-mini)
- --max-rows : optional, for testing smaller subsets

Environment requirements:
Your API keys must be defined in:

config/.env

with lines like:

OPENAI_API_KEY=your_key_here
GEMINI_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here

The script automatically loads environment variables and will raise an error if keys are missing.

## Stage 4 ‚Äî Bayesian Reasoning Models (analysis_pt2.py)

Stage 4 takes the full reasoning-coded dataset from Stage 3 and fits two sets of Bayesian hierarchical models:

1. A **logistic-normal compositional model** for the four content reasons:
   - fact
   - tone_reason
   - style
   - emotion

2. A set of **Bernoulli logistic hierarchical models** for stereotype-related outcomes:
   - stereo
   - avoid_stereo
   - other
   - mentions_stereotype

All models include random effects for:
- model
- scenario
- tone
- factor (occupation / food / hobby_profile)
- model √ó scenario interaction

This mirrors the structure used in Stage 2 but for explanation content rather than pronoun choice.

### Compositional Model (fact, tone_reason, style, emotion)

The four-part vector (fact, tone_reason, style, emotion) is transformed via an additive log-ratio (ALR) transform and modeled with a hierarchical multivariate normal:

fact*, tone_reason*, style*  ~  MVN( Œº + group-level effects , Œ£ )

emotion serves as the reference part in the ALR transform.  
Posterior samples are transformed back to compositions for interpretation.

Outputs include:
- Global mean compositions
- Model-level compositions
- Scenario-, tone-, and factor-level compositions
- Full cell-level compositions (scenario √ó model √ó tone √ó factor)
- Pairwise model differences in compositions

### Bernoulli Models (stereo, avoid_stereo, other, mentions_stereotype)

Each binary outcome y_i is modeled as:

logit(p_i) = Œ±
           + a_model
           + a_scenario
           + a_tone
           + a_factor
           + a_model√óscenario

with weakly informative priors and hierarchical standard deviations œÉ_‚ãÖ ~ HalfNormal.

Outputs include:
- Baseline probabilities for each model
- Model-level pairwise differences
- Scenario-, tone-, and factor-level probabilities
- Cell-level probabilities for every observed condition

### Example output directory

results/analysis_pt2_logitnormal/

Contains files such as:
- summary_global_content.csv
- summary_model_content.csv
- summary_scenario_content.csv
- summary_tone_content.csv
- summary_factor_content.csv
- summary_pairwise_model_content.csv
- cell_content_compositions.csv
- summary_model_stereo.csv
- summary_model_avoid_stereo.csv
- summary_model_other.csv
- summary_model_mentions_stereotype.csv
- Corresponding scenario/tone/factor summaries
- Cell-level stereotype probabilities
- Posterior trace files for all models

### How to run

python code/analysis_pt2.py \
    --infile data/reasons_full_compare/exp_full_LLM_scored.xlsx \
    --outdir results/analysis_pt2_logitnormal \
    --draws 2000 \
    --tune 2000 \
    --target_accept 0.97

Arguments:
- --infile : the fully coded reasoning dataset from Stage 3
- --outdir : directory to write all posterior summaries and trace files
- --draws : number of posterior draws
- --tune : warmup iterations
- --target_accept : NUTS acceptance target (increase if divergences occur)

The script automatically:
- Loads data
- Normalizes fields
- Fits ALR-based logistic-normal models for compositional data
- Fits Bernoulli logistic hierarchical models for stereotype outcomes
- Writes complete posterior summaries to the output directory
